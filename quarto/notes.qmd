---
title: Graph Wavenet reproduction (and some notes along the way)
date: 15 07 2024
bibliography: gwnetrepro.bib
---

Graph Wavenet [@wu2019graphwavenetdeepspatialtemporal] is a common baseline for testing your model against, and can be a good start when working with spatio temporal data, because it has a relatively simple architecture. I did not find the current implementations easy to work with, hence this repo.

### Plan

Currently can install everything except PyGT. So, next step is to modify the loader to return batches of PyG Data objects.

1. MVP with no PyGT.
2. Change GWnet to work on whole data, no subgraphs.
3. Prioritise TODOs from here on.

### TODOs

- [ ] Port code I already have to this repo.
- [ ] Identify the relevant bits that have to come in here.
- [ ] Ensure dataloader downloads data from reputable place (Zenodo?) and no pickles are used.
- [ ] Produce reproduction table for three metrics.
- [ ] Setup CI/CD and testing.
- [ ] Package indexed and installable via pip.

### Files to move over

#### Dataloaders
- datasets/metrla.py
- datasets/pemsbay.py

Currently using torch geometric temporal dataset classes. Change to something that loads easier into the format GWnet needs.
We want to provide more quality of life changes to the data downloading, unpacking, etc process.

- [ ] Remove interpolation? At least note that it is off.
- [ ] Remove having to load pickles.

#### Loss
- loss/supervised/*.py

Make explicit the masking, write about it.

#### Model
- model/subgraph.py
- layer/mixhop.py
- baseline

There is actually a mixhop layer in PyG now, so use that. Will also have to redo subgraph.py to take in the entire graph with only spatial edges, no subsampling, no heterogeneity. Should end up being much, much simpler.
Also, why not show the performance of a baseline? It's actually quite a bit stronger than the baseline that they show in the paper.
Note we don't need any sampling for this version, since the entire traffic graph goes in.
Also need to modify the model to use the vector embeddings as they have them in the paper. In our implementation we ignored them.

#### Training
- supervised/subgraph.py

Will likely need to rewrite and simplify the training script. Also using lightning might simplify the code quite a bit. But maybe drop it, if we want to minimise the dependencies.

#### Utils

Not sure if we need any of these, but most are helper functions for the experiments we did.

### Implementation notes

#### Data loading

Previously the code used to work with StaticGraphTemporalSignal as the iterator to the dataset. I am guessing the advantage to that is in case you are using a RNN type network, which is likely quite common for temporal data. For that type of application, the data must come as a temporal sequence.
For graph wavenet, we will train using minibatches that contain all the relevant temporal information as time series associated with a node. This means we don't necessarily need to adhere to the temporal sequence once, we can randomly sample and build our batches like that.

- [ ] Introduce PyGT and PyG libraries.

I begin the implementation by removing the dependency on PyGT. PyGT was difficult to install during RedyGraph, and it remains difficult to install to this day.

So what should I use instead of StaticGraphTemporalSignal?

PyG provides [instructions](https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html#creating-in-memory-datasets) for creating your custom graph dataset, similarly to Pytorch.
If the data is small enough to fit in RAM, then ```torch_geometric.data.InMemoryDataset``` is the recommended way to wrap the data, otherwise ```torch_geometric.data.Dataset``` is suggested.
The latter seems to be more general and you'd have to implement the logic behind creating individual graphs, maybe from a remote database or something like that.
The former requires you to implement the logic for acquiring the full dataset, e.g downloading it from the internet, and turning it into a list of ```Data``` objects. They are then saved locally, and I guess PyG takes care for serving it from there.
There already is a Zenodo [record](https://zenodo.org/records/5724362) for METR-LA and PEMS-BAY that somebody created, so let's use that as our source, and create an in-memory dataset class for it. Both datasets are less than 200Mb in size.
Perhaps I should contribute this to PyG, and also create a different record that does not serve any ```pickle``` artefacts, which are known to be unsafe, after all.

![Background [photo](https://www.pexels.com/photo/pickles-on-a-glass-jar-8599631/) from Pexels.](./images/pickle_poster.png){}

- [ ] Build two in-memory dataset loaders for METR-LA and PEMS-BAY, and push upstream to PyG.
	- [ ] To push to upstream, I need to write the docstring as done in the other datasets.
- [ ] Find a better way to distribute the adjacency matrix than a pickle.

Anyway, I have opened that pickle in the past and I know it is safe, so let's build the loader on top of that and modify it later.
Here are the functions I need to implement:

```
InMemoryDataset.raw_file_names()
InMemoryDataset.processed_file_names()
InMemoryDataset.download()
InMemoryDataset.process()
```

To obtain the data we will use the provided ```torch_geometric.data.download_url``` function.

:::{.callout-note}
While implementing a test, I noticed that pytest is set to turn warnings into errors by default, and changed it to ignore a deprecation warning from an upstream. It was about deprecating some kind of string representation which is used in PyG.
I wonder whether there is a better way of handling this, without ignoring deprecation warnings? Do we generally want to turn every warning into an error?
:::

:::{.callout-note}
How do you manage the location of downloaded datasets? That is, when a repo is downloaded, which in turn should download some data that gets stored locally. How do you control the filepaths when you don't have access to the local env? Ideally I wouldn't want to download these csv files always in the location from which the script is run - that would pollute the user's directories.
:::

The Zenodo upstream for METR-LA contains a csv that looks like this for some two nodes.

:::{.callout-important}
A datapoint of 0.0 actually represents a missing value, not a traffic speed of 0.
:::

```{=html}
<table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Time</th>      <th>773869</th>      <th>767541</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>2012-03-01 00:00:00</td>      <td>64.375000</td>      <td>67.625000</td>    </tr>    <tr>      <th>1</th>      <td>2012-03-01 00:05:00</td>      <td>62.666667</td>      <td>68.555556</td>    </tr>    <tr>      <th>2</th>      <td>2012-03-01 00:10:00</td>      <td>64.000000</td>      <td>63.750000</td>    </tr>    <tr>      <th>3</th>      <td>2012-03-01 00:15:00</td>      <td>0.000000</td>      <td>0.000000</td>    </tr>    <tr>      <th>4</th>      <td>2012-03-01 00:20:00</td>      <td>0.000000</td>      <td>0.000000</td>    </tr>  </tbody></table>
```

There are in total 207 nodes. The adjacency matrix is in the pickle, which contains all sensor ids, a dictionary map from id to index ranging from 0 to 206, and a numpy array with the edge weights. I think this can alternatively be easily represented as a json or yaml list of the edges from id, to id. This could avoid having to deal with pickles.

So, creating an ```InMemoryDataset``` might be a little more challenging than I thought.
The thing is, the set of all ```Data``` samples are very big. The source data is small. This is because you end up not only copying the adjacency matrix over 30k times (207*207*30000*64bits~10Gb), once for each Data object, but you also copy the time series 12 times as well (64bits * 30000 steps * 12 * 2 features * 207 nodes ~ 1Gb), because each time measurement appears in about 12 nodes. The data is still rather small overall though. Actually, all Data objects come out to about 3Gb, because we are using sparse edge indices. Should be fine, so why the OOM? Because I was trying to torch.save numpy arrays. Not sure why it fails so spectacularly.
Still, the memory usage spikes to about 10Gb RAM for saving the model.

- [ ] Check if can remove numpy dependencies.

Looks like the dataset might work any moment now. Project left at running pre-commit. I've already made an initial commit. Might need to relax some pre-commit checks.

:::{.callout-note}
What are library stubs, and how do I install them? Why do I need them? For now I just tell mypy not to ignore missing imports.
:::

Ok, the dataset can now be served, it seems. There are still some ends to tie up in that code, but let's leave it for now, it looks like it works correctly.

#### Model and training loop

Now let's build the graph wavenet model. I will first adapt my existing code, which implements the forward and backward diffusions, but not the adaptive adjacency matrix.

::: {layout-ncol=2}

![Graph wavenet architecture diagram from the [paper](https://arxiv.org/pdf/1906.00121)](./images/gwnet_arch.png)

![Architecture diagram you can find in their code. As far as I recall it is not actually what they have implemented, but contains a bit more detail than the paper diagram.](./images/gwnet_code_diagram.png)

:::

:::{.callout-note}
The official implementation differs somewhat from the diagram. In the code they have added convolutions along the residual and skip connections. Maybe that is common practice? It is not mentioned in the paper, and there is no appendix in the Arxiv version.
:::

There are two specific components to this architecture, denoted as TCN and GCN in the diagram. These are the temporal (causal) convolution, and the graph convolution.

![Diagram of a series of causal convolutions. In this architecture, the sequence of dilated convolutions are applied in subsequent layers.](./images/tcn_diagram.png)

TCN is motivated in the Wavenet [paper](https://arxiv.org/abs/1609.03499) as a way to aggregate information from an exponentially increasing receptive field. However, in graph wavenet it seems to have been applied with dilations of 1,2,1,2,..., so only aggregating information from a small number of timesteps back.
I am not sure why the authors have chosen not to retreive information from a larger temporal context, as is done in the Wavenet paper.
The equation for the 'Gated TCN' component is the following:

\begin{equation}
    \mathbf{h} = g(\mathbf{\Theta}_1 * \mathbf{X} + \mathbf{b}) \odot \sigma(\mathbf{\Theta}_2 * \mathbf{X} + \mathbf{c})
\end{equation}

Here, $g(\cdot)$ and $\sigma(\cdot)$ are the hyperbolic tan and sigmoid activations, $*$ denotes a convolution (the causal convolution), and $\odot$ is the elementwise product. The input $\mathbf{X}$ is of shape $\mathbb{R}^{N \times D \times S}$ for $N$ nodes, feature dimensionality $D$, and $S$ timesteps back. For example, our METR-LA datapoints will have shape $\mathbb{R}^{207 \times 2 \times 12}$. Each node at a given timestep has two features - the traffic speed, and a number with periodicity of 288 steps (24 hours), which provides context for the time of day.

The GCN module aggregates information between nodes along the traffic network.
The paper introduces three components to the GCN module.
Two of these are part of the so-called 'diffusion convolution' [@li2018diffusionconvolutionalrecurrentneural], which fundamentally tells us which nodes to include, and with what weight, when computing the node state update. It is computed by taking powers of the adjacency matrix.
This will include edges from a central node to nodes that are 'reachable' after a certain number of hops, reminiscent of diffusion along the edges.
This is done once along the direction of the edges, and once against the direction of the edges (by using $\mathbf{A}^{\top}$ instead of $\mathbf{A}$), and these are called the 'forward' and 'backward' diffusion.

In addition to the diffusion convolution, an 'adaptive' adjacency matrix is also added.
Each node of the graph is assigned an embedding $\mathbf{E} \in \mathbb{R}^{c}$ which is learned during the training phase, and the adaptive adjacency matrix can be computed as

\begin{equation}
    \tilde{\mathbf{A}}_{adp} = \text{SoftMax}(\text{ReLU}(\mathbf{E}_1 \mathbf{E}_2^{\top}))
\end{equation}

Where $\mathbf{E}_1 \mathbf{E}_2^{\top}$ is an inner product. The adjacency matrix is normalised due to the softmax. This adaptive adjacency matrix is supposed to infer the 'transition matrix of a hidden diffusion process'.
It makes sense, I imagine, that you can learn the graph wiring which yields the best performance for your task.
When you use this matrix in addition to the original adjacency, then it might be learning a diff between the current graph, and possibly more optimal version?
It would be interesting to see whether the adjacency matrix can learn to somewhat recover the original road network, if used on its own.

The full equation with diffusion in both directions and the adaptive adjacency is

\begin{equation}
    \mathbf{Z} = \sum_{k=0}^{K}\mathbf{P}^k_f \mathbf{X} \mathbf{W}_1 + \mathbf{P}^k_b \mathbf{X} \mathbf{W}_2 + \tilde{\mathbf{A}}^k_{apt} \mathbf{X} \mathbf{W}_3
\end{equation}

Here $\mathbf{W}_{1,2,3}$ are parameters, $\mathbf{P}$ are the forward and backward diffusion transition matrices, $\tilde{\mathbf{A}}_{apt}$ is the adaptive adjacency matrix, and $\mathbf{X}$ is the input. The adjacency matrices are all raised to the power $k$.
The forward and backward diffusion matrices are computed by normalising the adjacency matrix and interpreting each unit weight as carrying a unit of probability of transition.

\begin{equation}
    \mathbf{P}_f = \frac{\mathbf{A}}{\sum_j\mathbf{A}_{ij}}
\end{equation}

\begin{equation}
    \mathbf{P}_b = \frac{\mathbf{A^{\top}}}{\sum_j\mathbf{A^{\top}}_{ij}}
\end{equation}

The original graph wavenet implementation works on the entire road network without subsampling. Maybe we can implement subsampling, but for now let's follow how they've done it originally.
In principle, GWnet has a limited receptive field, so anything more than that amounts to batching (right?).
Allowing spatial subsampling should be nice for when training on bigger graphs.

![Let's refactor some of the gems in the original implementation. This is difficult to read, but also exceptions should be used for exceptional circumstances, and not for controlling execution in high performance code.](./images/tryexcept.png)

The TCN layer can be implemented using a Conv1d or Conv2d on the 2-by-12 'image'.
Latter is how they've done it in the original implementation.
It is a bit difficult to read though.
They say that the causality can be achieved by padding the input with eros on one side.
Note how they mention the increasing dilation factors for capturing exponential receptive field, but they then explicitly don't do that and never motivate their choice.

A design choice they've made is to set the padding, stride, and dilation parameters such that the input sequence gets gradually reduced to 1.
I suppose I'll just do the same.
If you set the Conv1D kernel size to 2, dilation to 1, and add no padding, the output sequence will be shorter by 1.
If you do the same, but dilation 2, the output will be shorter by 2.
They keep all kernel sizes 2, and have a sequence of dilations $\left[1,2,1,2,1,2,1,2\right]$ for their 8 layers.
This means that for an input sequence of 13, the final layer will be dealing with an input sequence of 1.
Hence, our TCN-a and TCN-b will be made of ```nn.Conv1d(32, 32, ks=2, dilation=d)``` where the dilation will alternate between 1 and 2.

For the GCN layer we can precompute the powers of the diffusion matrix and reuse, since the adjacency is the same (the entire road network).
I can implement this as a caching mechanism, the same way the PyG [GCN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html) layer does.
There it caches the adjacency normalisation, but we can use a similar mechanism to store the powers of the adjacency.
These adjacencies won't change unless the graph structure changes, which we do not expect.

But before I embark on implementing a new layer, could we possibly use the [GDN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.transforms.GDC.html) PyG implementation of "Diffusion Improves Graph Learning" [@gasteiger2022diffusionimprovesgraphlearning]?
It seems to do a bunch of other things like sparsification as built-in, so maybe not the right choice.
How about the [MixHop](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.MixHopConv.html) module, introduced in [@abuelhaija2019mixhophigherordergraphconvolutional]? This one simply does the following operation:

\begin{equation}
    \mathbf{Z} = \Vert_{p \in P} \left( \hat{\mathbf{D}}^{-\frac{1}{2}}\hat{\mathbf{A}}\hat{\mathbf{D}}^{-\frac{1}{2}} \right)^{p}\mathbf{X}\mathbf{\Theta}
\end{equation}

Where $\Vert$ is concatenation. We could reshape the concatenated vector and sum-reduce, yielding:

\begin{equation}
    \mathbf{Z} = \sum_{p \in P} \left( \hat{\mathbf{D}}^{-\frac{1}{2}}\hat{\mathbf{A}}\hat{\mathbf{D}}^{-\frac{1}{2}} \right)^{p}\mathbf{X}\mathbf{\Theta}
\end{equation}

But unfortunately we can't seem to change the normalisation, it is built-in symmetric.
Note how close this is to the diffusion convolution:

\begin{equation}
    \mathbf{Z} = \sum_{k=0}^{K}\mathbf{P}^k \mathbf{X} \mathbf{W}_1
\end{equation}

If we were to set $P = (0,1,2)$, and replace $\left( \hat{\mathbf{D}}^{-\frac{1}{2}}\hat{\mathbf{A}}\hat{\mathbf{D}}^{-\frac{1}{2}} \right)^{p}$ with $\left(\mathbf{A}\mathbf{D}^{-1}\right)^p$, then we should have exactly the diffusion convolution as defined in the paper.
Maybe implementing the exact Diffusion module can be future work, let's just use MixHop.
The normalisation won't be right, but the support for the convolution should be the same.

![Inspiring productivity: Author unknown](./images/mediocrates.png)

- [ ] Implement Diffusion conv with the correct normalisation.

So to summarise, use ```torch_geometric.nn.conv.MixHopConv``` as originally implemented.
It transforms $(*, C_{in}) \longrightarrow (*, |P|C_{out})$ due to concatenation.
Follow up the layer with reshape $(*, |P|, C_{out})$ then sum along $|P|$ to get back $(*, C_{out})$.

:::{.callout-note}
In presentation, would be worth to explain PyG batching.
:::

Annoyingly, I'm trying to use MixHop, but it doesn't know what to do with nodes that have more than one feature dimension.
Ok, so looks like we'll be implementing Diffusion from scratch.
Might as well go through how to implement a PyG module.. Mediocrates has lost on this occasion.

MixHop is a mess when dealing with time series in the nodes. So build a custom diffusion using matrix multiplication...

- [ ] Implement two versions of Diffusion. One in the graph and one in the node view.

Ok, but on the other hand, building Diffusion in the graph view, you run into an issue if you also use PyG Data types.
This is because the way PyG does batching is to create block-diagonal adjacency instead of creating a batch dimension.
The PyG way is good for when the batched graphs have different sizes, but in my case I'd need to either modify the adjacency for the correct batch size, or stop using PyG Data.
I don't like these two solutions, because they are overly specific to this problem, and the code will start to look a lot like the original GWnet repo.
On the other hand, I could implement a Diffusion PyG module that works with the (C,T) feature tensors, which MixHop doesn't seem to like.
This approach seems to be the best in terms of design, because my modification is limited to subclassing MessagePassing for my particular problem.
It is also educational, because I get to introduce how to build a MessagePassing GNN layer.
However, unless I implement caching of the adjacency powers, every layer would have to compute three steps of message passing.
Luckily, I think it would be straightforward to cache, since you always pass the edge index to a graph conv layer, hence you can pass a different adjacency than the input's.

Let's break down the full equation:

\begin{equation}
    \mathbf{Z} = \sum_{k=0}^{K}\mathbf{P}^k_f \mathbf{X} \mathbf{W}_{k}^{(f)} + \mathbf{P}^k_b \mathbf{X} \mathbf{W}_{k}^{(b)} + \tilde{\mathbf{A}}^k_{apt} \mathbf{X} \mathbf{W}_{k}^{(apt)}
\end{equation}

The new state of a node $\mathbf{Z}$ is the sum of $K+1$ terms of the form $\mathbf{A}_{k}\mathbf{X}\mathbf{W}_{k}$, for some adjacency $\mathbf{A}_k$ and dense update weights $\mathbf{W}_k$.
For $k=0$ the graph convolution becomes simply a Linear update to each node, since $\mathbf{A}^0 = \mathbf{I}$, hence the three terms become:

\begin{equation}
    \mathbf{X}\mathbf{W}^{(f)}_0 + \mathbf{X}\mathbf{W}^{(b)}_0 + \mathbf{X}\mathbf{W}^{(adp)}_0 = \mathbf{X}\left( \mathbf{W}^{(f)}_0 + \mathbf{W}^{(b)}_0 + \mathbf{W}^{(adp)}_0\right) = \mathbf{X}\mathbf{W}
\end{equation}

I'm not sure why the zeroth term is added to the sum, but it looks like we can instead start the sum from $k=1$ and add one Linear layer that updates each node without the graph signal. Hence, the update equation becomes:

\begin{equation}
    \mathbf{Z} = \sum_{k=1}^{K}\left[\mathbf{P}^k_f \mathbf{X} \mathbf{W}_{k}^{(f)} + \mathbf{P}^k_b \mathbf{X} \mathbf{W}_{k}^{(b)} + \tilde{\mathbf{A}}^k_{apt} \mathbf{X} \mathbf{W}_{k}^{(apt)}\right] + \mathbf{X}\mathbf{W}_0 + \mathbf{b}
\end{equation}

Where $\mathbf{X}\mathbf{W}_0$ is the update from $k=0$ as discussed above, and $\mathbf{b}$ is the layer's bias.
The term $\mathbf{X}\mathbf{W}_0 + \mathbf{b}$ together can be built as a ```nn.Linear``` with ```bias=True```.

Luckily, if we look at how the PyG [GCNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html) layer works, we notice that you can set ```normalize=False```, which configures the layer to only do:

```python
def forward(self, x: Tensor, edge_index: Adj,
            edge_weight: OptTensor = None) -> Tensor:

    x = self.lin(x)

    # propagate_type: (x: Tensor, edge_weight: OptTensor)
    out = self.propagate(edge_index, x=x, edge_weight=edge_weight)

    if self.bias is not None:
        out = out + self.bias

    return out
```

So further setting ```bias=False``` means the layer simply does $\mathbf{A}\mathbf{X}\mathbf{W}$, where the weights associated with $\mathbf{A}$ are passed as ```edge_weight```.
This means that our graph convolution module can be built up from $KM$ ```GCNConv``` Modules, each using a separate adjacency matrix.
The integer $M \in \left[1, 3\right]$ is how many of the adjacencies: forward, backward diffusion and adaptive the model is using.
These adjacency matrices could be stored in the parent Module, and our custom graph conv Module could be instantiated with a reference to its parent.
Note also that on every forward pass we would have to compute $\mathbf{A}_{apt}$, because backpropagation will have changed the node embeddings.
And, of course, we would also have to register the node embedding matrix as a ```Parameter``` which requires grad.
I wonder how this would change whether the model is inductive or transductive?
You definitely won't be able to use a new node's embeddings.

:::{.callout-note}
Ok, so all the GCNs in our Graph Wavenet take the input and perform the propagation operation.
These operations can be done in parallel, similarly to multihead attention.
So, is there a way to do this easily in parallel, or does this start to enter the realm of optimisation..?
:::

Passing the adjacency into the GWnet constructor seemed as the way to precompute, but I'd have to pass the adjacency of a BatchedData, otherwise the batching is not taken care of.
It might be easier to precompute the diffusion adjacencies on each forward pass, so that you directly get access to the batched adjacency index and weights.
It also means that this should trivially work if subgraphing, because the precomputation is done once the subgraph is sampled.
But in either case, there is no getting away from the fact that the adaptive adjacency has to be computed on each forward, that it is not batched, and that I will likely have to make it fit the batch manually.
Luckily, there is ```torch.block_diag```, which might be able to do it easily, but it does not support sparse inputs [yet](https://github.com/pytorch/pytorch/issues/31942) and might be inefficient to go to dense for the entire batch, then to sparse again.

- [ ] Implement adjacency precomputation per forward pass
- [ ] Implement adaptive adjacency computation per forward pass using ```torch.block_diag```.
- [ ] Look whether I can contribute to [https://github.com/pytorch/pytorch/issues/31942](https://github.com/pytorch/pytorch/issues/31942)

Ok so today I built the adjacency precomputation, but I'm not sure whether the powers are correct, because no new edges are added to the index.
Looks like taking powers of sparse matrices might be a little more involved than simply using Python syntax.

- [ ] Write GH Pytorch issue requesting powers for sparse matrix?
- [ ] GH Pytorch sparse division by vector?

Taking the power of the dense batch adjacency and then saving it once?
But it also looks like we can take powers of the COO matrix, so maybe it will work straightaway if I pass the real COO matrix.
The adaptive bit I'd have to compute densely and take powers, but then I can turn into a batch block diagonal using utility functions.
So maybe the way to go is dense powers of forward and backward once at beginning of training, then per forward pass update the adaptive adjacency.

I've precomputed the adjacencies, but still there is the problem of GCN not knowing how to handle (N,C,L) tensors.
I'll likely have to implement a GCN that does that correctly.
By precomputing, I think we can run the sum concurrently, but I don't know how to do that.
Instead, I could run the newly implemented GCN as a loop over the k-hops.

- [ ] Build a custom GCN layer that can handle the time series node type.
- [ ] Open PyG issue for (N, L, C) features breaking GCNConv.

Ok so my layer seems to work if using ```torch.nn.Linear``` with transposition, but ```torch_geometric.nn.dense.Linear``` with transposition doesn't work?
What's going on?
Anyway, it looks like it's doing the correct thing after essentially replacing the PyG Linear with the torch Linear in my custom layer.
Also fixed bug when computing the transition matrix, so now it makes sense when I check the output.

At this point, the residual module looks correct and it produces outputs of a sensible shape.
Ok so what are the skip convolutions, and how do I select the correct vector to add to the skip output?
I've chosen to add to the skipped output the value of only the last element of the temporal sequence.
This is in part because that's how I interpret the official implementation.
In this way, all previous timesteps act as context for the final one, a bit like how ViT has a special classification token that holds the output, and uses the other tokens as context.

#### Training script

I'll use torch lightning.

- [ ] Introduce torch lightning.

There are a couple of points to note about the training procedure.
First, loss is not calculated on the missing values, which in METR-LA are denoted as 0.0.
Second, the MAE in literature is calculated in the unnormalised range, so keep that in mind when reporting it.

Ok so I've written a little training script, and linted the model code.
Yet to find out whether the training does anything useful.
Next step might be to plot the adaptive adjacency as the training progresses.
Also profile the training to see whether I get any bottlenecks as I used to before.

I fixed a NaN bug where having all 0 inputs breaks.
Forgot to shuffle the dataset, which completely throws it off.
And added the missing skip connections to the residual modules.
Now it looks like it does something useful.
However, adding the adaptive adjacency massively increases the GPU cost.
I will likely need to optimise it, I can only train up to 8 batch size on 12Gb of VRAM at the moment.
Not sure off the top of my head where the massive memory footprint comes from, maybe I didn't make the matrix sparse?

- [ ] Write the validation and test steps.
- [ ] Parameterise the reproduction script.
- [ ] Debug/optimise the adaptive adjacency.
- [ ] Visualise the adaptive adjacency evolution using nx in tensorboard.
- [ ] Add PEMS-BAY dataset.


### References

::: {#refs}
:::
